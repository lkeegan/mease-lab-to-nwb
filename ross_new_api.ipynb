{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface pipeline for Mease Lab - CED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import getenv\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.toolkit as st\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.exporters as sx\n",
    "import probeinterface as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mease_lab_to_nwb.convert_ced.cednwbconverter import quick_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import resource\n",
    "\n",
    "# print function that timestamps the output & displays max memory usage\n",
    "def tprint(string=\"\"):\n",
    "    max_mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1e6\n",
    "    print(f\"|| {datetime.datetime.now():%H:%M:%S} || {max_mem} || {string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load bin recording instead of smrx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"Loading recording\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_file = Path(\n",
    "    r\"/mnt/sds-hd/sd19b001/PainData/Corrected_Channel_Map/L6/Cortex/20.8.21/KS2/m6.bin\"\n",
    ")\n",
    "recording_prb = \"cambridge_neurotech_H3.prb\"\n",
    "sampling_frequency = 3.003003003003003e04\n",
    "data_type = \"int16\"\n",
    "numChan = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikeinterface_folder = bin_file.parent / \"liam_new_api\"\n",
    "spikeinterface_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rhd channels should have already been selected thanks to smrx2bin\n",
    "recording = se.BinaryRecordingExtractor(\n",
    "    bin_file, sampling_frequency, numChan, data_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probe file\n",
    "probegroup = sp.read_prb(recording_prb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add probe file to recording\n",
    "recording = recording.set_probegroup(probegroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like the closest to get_shared_channel_property_names:\n",
    "recording.get_property_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# note: previously get_traces returned array[channel][time]\n",
    "# with new spikeinterface API order is swapped: array[time][channel]\n",
    "plt.plot(recording.get_traces(end_frame=30000)[:, 0], label=\"channel 0\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.plot_probe_map(recording)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num channels: {recording.get_num_channels()}\")\n",
    "print(f\"Channel ids: {recording.get_channel_ids()}\")\n",
    "print(f\"Sampling rate: {recording.get_sampling_frequency()}\")\n",
    "print(\n",
    "    f\"Duration (s): {recording.get_num_frames() / recording.get_sampling_frequency()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get LFPs from MultiChannel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"LFPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_lfp = recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: no \"resample\" method in new API: skipping this line\n",
    "# recording_lfp = st.preprocessing.resample(recording_lfp, resample_rate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_filter = True\n",
    "freq_min_hp = 0.1\n",
    "freq_max_hp = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_filter:\n",
    "    recording_lfp = st.preprocessing.bandpass_filter(\n",
    "        recording_lfp, freq_min=freq_min_hp, freq_max=freq_max_hp\n",
    "    )\n",
    "else:\n",
    "    recording_lfp = recording_lfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording_lfp is a copy of the original recording with an additional bandpass_filter pre-processing step\n",
    "# the pre-processing is \"lazy\", which means it isn't done until needed, e.g. when get_traces is called:\n",
    "plt.figure()\n",
    "plt.plot(recording.get_traces(end_frame=3000)[:, 0], label=\"channel 0\")\n",
    "plt.plot(recording_lfp.get_traces(end_frame=3000)[:, 0], label=\"channel 0 LFP\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"Pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.preprocessing.preprocessers_full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_filter = True\n",
    "apply_cmr = True\n",
    "freq_min_hp = 600\n",
    "freq_max_hp = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st.preprocessing.common_reference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_filter:\n",
    "    recording_processed = st.preprocessing.bandpass_filter(\n",
    "        recording, freq_min=freq_min_hp, freq_max=freq_max_hp\n",
    "    )\n",
    "else:\n",
    "    recording_processed = recording\n",
    "\n",
    "if apply_cmr:\n",
    "    recording_processed = st.preprocessing.common_reference(recording_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stub recording for fast testing; set to False for running processing pipeline on entire data\n",
    "stub_test = False\n",
    "nsec_stub = 60\n",
    "\n",
    "if stub_test:\n",
    "    recording_processed = recording_processed.frame_slice(\n",
    "        0, int(nsec_stub * recording_processed.get_sampling_frequency())\n",
    "    )\n",
    "    recording_lfp = recording_lfp.frame_slice(\n",
    "        0, int(nsec_stub * recording_lfp.get_sampling_frequency())\n",
    "    )\n",
    "\n",
    "print(f\"Original signal length: {recording.get_num_frames()}\")\n",
    "print(f\"Processed signal length: {recording_processed.get_num_frames()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(24, 8))\n",
    "ax0.set_title(\"Recording\")\n",
    "sw.plot_timeseries(recording, time_range=[4, 8], ax=ax0)\n",
    "ax1.set_title(\"LFP\")\n",
    "sw.plot_timeseries(recording_lfp, time_range=[4, 8], ax=ax1)\n",
    "ax2.set_title(\"Processed\")\n",
    "sw.plot_timeseries(recording_processed, time_range=[4, 8], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run spike sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Installed sorters:', ss.installed_sorters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorter_list = [\n",
    "#     \"kilosort2_5\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorter_params = dict()\n",
    "# for sorter in sorter_list:\n",
    "#     # start with defaults\n",
    "#     params = ss.get_default_params(sorter)\n",
    "#     # make changes\n",
    "#     params['chunk_mb'] = 2000\n",
    "#     params['n_jobs_bin'] = 16\n",
    "#     # print params\n",
    "#     print(f\"\\n\\n{sorter} params description:\")\n",
    "#     pprint(ss.get_params_description(sorter))\n",
    "#     print(f\"\\n\\n{sorter} params:\")\n",
    "#     pprint(params)\n",
    "#     sorter_params[sorter] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.available_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.run_sorters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set this to True to use the local SSD on the cluster node to store temporary files\n",
    "# if False, all data is written to SDS, which works fine but is a bit slower\n",
    "# BUT: local SSD only has 120GB of space on most GPU nodes - for large recordings this may not be enough space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_scratch_dir = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting_working_folder = spikeinterface_folder / 'simple_bin_output'\n",
    "# if use_scratch_dir:\n",
    "#     local_scratch_dir = Path(getenv(\"TMPDIR\"))\n",
    "#     tprint(f\"using local_scratch_dir = {local_scratch_dir}\")\n",
    "#     sorting_working_folder = spikeinterface_folder / 'simple_bin_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"Running sorters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting_outputs = ss.run_sorters(\n",
    "#     sorter_list=sorter_list,\n",
    "#     working_folder=sorting_working_folder,\n",
    "#     recording_dict_or_list=dict(rec0=recording_processed),\n",
    "#     sorter_params=sorter_params,\n",
    "#     mode=\"keep\", # \"overwrite\" to overwrite # change to \"keep\" to avoid repeating the spike sorting\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sorting_outputs` is a dictionary with (\"rec_name\", \"sorter_name\") as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tprint(\"Finished running sorters\")\n",
    "# for result_name, sorting in sorting_outputs.items():\n",
    "#     rec_name, sorter = result_name\n",
    "#     print(f\"{sorter} found {len(sorting.get_unit_ids())} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = se.PhySortingExtractor(\n",
    "    \"/mnt/sds/PainData/Corrected_Channel_Map/L6/Cortex/20.8.21/KS2/simple_bin_output/rec0/kilosort2_5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = si.extract_waveforms(\n",
    "    recording_processed,\n",
    "    sorting,\n",
    "    \"temp_waveforms\",\n",
    "    n_jobs=6,\n",
    "    total_memory=\"1G\",\n",
    "    verbose=True,\n",
    "    progress_bar=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"post extract_waveforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx.export_to_phy(\n",
    "    we, \"export_to_phy\", n_jobs=6, total_memory=\"2G\", verbose=True, progress_bar=True\n",
    ")\n",
    "tprint(\"post export_to_phy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = sx.export_to_phy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"compute templates\")\n",
    "templates = st.postprocessing.get_unit_templates(\n",
    "    recording_processed, sorting, n_jobs=16, chunk_mb=2000, **postprocessing_params\n",
    ")\n",
    "\n",
    "tprint(\"compute EC features\")\n",
    "ec = st.postprocessing.compute_unit_template_features(\n",
    "    recording_processed,\n",
    "    sorting,\n",
    "    n_jobs=16,\n",
    "    chunk_mb=2000,\n",
    "    feature_names=ec_list,\n",
    "    as_dataframe=True,\n",
    "    memmap=True,\n",
    ")\n",
    "## compute QCs\n",
    "# qc = st.validation.compute_quality_metrics(sorting, recording=recording_processed,\n",
    "#                                           metric_names=qc_list, as_dataframe=True, memmap = False)\n",
    "\n",
    "# export to phy example\n",
    "# pprint(postprocessing_params)\n",
    "if sorter == \"kilosort2_5\":\n",
    "    # pprint(postprocessing_params)\n",
    "    recompute_info = True\n",
    "    phy_folder = spikeinterface_folder / \"phy\" / sorter\n",
    "    phy_folder.mkdir(parents=True, exist_ok=True)\n",
    "    tprint(\"Exporting to phy\")\n",
    "    st.postprocessing.export_to_phy(\n",
    "        recording_processed,\n",
    "        sorting,\n",
    "        phy_folder,\n",
    "        compute_pc_features=False,\n",
    "        verbose=True,\n",
    "        memmap=True,\n",
    "        recompute_info=True,\n",
    "        n_jobs=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Post-processing: extract waveforms, templates, quality metrics, extracellular features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set postprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.postprocessing.template_metrics.pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing params\n",
    "postprocessing_params = st.postprocessing.get_common_params()\n",
    "postprocessing_params[\"verbose\"] = True\n",
    "postprocessing_params[\"recompute_info\"] = True\n",
    "postprocessing_params[\"memmap\"] = True\n",
    "postprocessing_params[\n",
    "    \"max_spikes_per_unit\"\n",
    "] = 1000  # with None, all waveforms are extracted\n",
    "pprint(postprocessing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set quality metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "qc_list = st.validation.get_quality_metrics_list()\n",
    "print(f\"Available quality metrics: {qc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of qc\n",
    "qc_list = [\n",
    "    \"num_spikes\",\n",
    "    \"firing_rate\",\n",
    "    \"presence_ratio\",\n",
    "    \"isi_violation\",\n",
    "    \"amplitude_cutoff\",\n",
    "    \"snr\",\n",
    "    \"max_drift\",\n",
    "    \"cumulative_drift\",\n",
    "    \"silhouette_score\",\n",
    "    \"isolation_distance\",\n",
    "    \"l_ratio\",\n",
    "    \"noise_overlap\",\n",
    "    \"nn_hit_rate\",\n",
    "    \"nn_miss_rate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set extracellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracellular features\n",
    "ec_list = st.postprocessing.get_template_features_list()\n",
    "print(f\"Available EC features: {ec_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess all sorting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    tprint(f\"Postprocessing recording {rec_name} sorted with {sorter}\")\n",
    "    tmp_folder = local_scratch_dir / 'tmp_ced' / sorter\n",
    "    tmp_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # set local tmp folder\n",
    "    sorting.set_tmp_folder(tmp_folder)\n",
    "     \n",
    "       \n",
    "    # pprint(postprocessing_params)\n",
    "    \n",
    "    st.postprocessing.\n",
    "    \n",
    "    tprint(\"compute waveforms\")\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(recording_processed, sorting, \n",
    "                                                     n_jobs=16, chunk_mb=2000, **postprocessing_params)\n",
    "    \n",
    "    tprint(\"compute templates\")\n",
    "    templates = st.postprocessing.get_unit_templates(recording_processed, sorting, n_jobs=16, chunk_mb=2000, **postprocessing_params)\n",
    "    \n",
    "    tprint(\"compute EC features\")\n",
    "    ec = st.postprocessing.compute_unit_template_features(recording_processed, sorting, n_jobs=16, chunk_mb=2000,\n",
    "                                                          feature_names=ec_list, as_dataframe=True, memmap = True)\n",
    "    ## compute QCs\n",
    "    #qc = st.validation.compute_quality_metrics(sorting, recording=recording_processed, \n",
    "    #                                           metric_names=qc_list, as_dataframe=True, memmap = False)\n",
    "    \n",
    "    # export to phy example\n",
    "    # pprint(postprocessing_params)\n",
    "    if sorter == \"kilosort2_5\":\n",
    "       # pprint(postprocessing_params)\n",
    "        recompute_info = True\n",
    "        phy_folder = spikeinterface_folder / 'phy' / sorter\n",
    "        phy_folder.mkdir(parents=True, exist_ok=True)\n",
    "        tprint(\"Exporting to phy\")\n",
    "        st.postprocessing.export_to_phy(recording_processed, sorting, phy_folder, compute_pc_features=False, verbose=True, memmap = True, recompute_info = True, n_jobs=16)\n",
    "        #st.postprocessing.export_to_phy(recording_processed, sorting, phy_folder, verbose=True, compute_pc_features=False, compute_amplitudes=False, memmap = False, recompute_info = True, n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_kilosort = sorting_outputs[(\"rec0\", \"kilosort2_5\")]\n",
    "print(f\"Properties: {sorting_kilosort.get_shared_unit_property_names()}\")\n",
    "print(f\"Spikefeatures: {sorting_kilosort.get_shared_unit_spike_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprint(\"Postprocessing done\")\n",
    "# stop here for now:\n",
    "raise Exception(\"Stopping notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Phy-curated data back to SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!phy template-gui Z:\\PainData\\m365\\10min\\phy\\kilosort2_5\\params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_folder = r\"Z:\\PainData\\Corrected_Channel_Map\\L6\\Cortex\\16.12.20\\phy\\kilosort3\"\n",
    "recording_phy = se.PhyRecordingExtractor(phy_folder)\n",
    "sorting_curated = se.PhySortingExtractor(phy_folder)\n",
    "sorting_phy = se.PhySortingExtractor(phy_folder, exclude_cluster_groups=[\"noise\"])\n",
    "print(f\"Units after manual curation: {len(sorting_curated.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_units = []\n",
    "for u in sorting_phy.get_unit_ids():\n",
    "    if sorting_phy.get_unit_property(u, \"quality\") == \"good\":\n",
    "        good_units.append(u)\n",
    "sorting_good = se.SubSortingExtractor(sorting_phy, unit_ids=good_units)\n",
    "print(good_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?st.curation.threshold_num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_curated = st.curation.threshold_num_spikes(\n",
    "    sorting_curated, threshold=50, threshold_sign=\"less\"\n",
    ")\n",
    "print(f\"Units after num spikes curation: {len(sorting_curated.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_phy = recording_phy.get_traces(end_frame=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(tr_phy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Quick save to NWB; writes only the spikes and lfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To complete the full conversion for other types of data, either\n",
    "###    1) Run the external conversion script before this notebook, and append to it by setting overwrite=False below\n",
    "###    2) Run the external conversion script after this notebook, which will append the NWBFile you make here so long as overwrite=False in the external script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your NWBFile and decide where you want it saved\n",
    "nwbfile_path = r\"Z:\\PainData\\Corrected_Channel_Map\\L6\\Cortex\\16.12.20\\phy\\kilosort3\\m380_NewChanMap.nwb\"\n",
    "\n",
    "# Enter Session and Subject information here\n",
    "session_description = \"m380 spikes without TTL - 10min test\"\n",
    "\n",
    "# Manually insert the session start time\n",
    "session_start = datetime(2020, 10, 8)  # (Year, Month, Day)\n",
    "session_start_time = \"2020-12-16T16:30:00\"\n",
    "\n",
    "# Choose the sorting extractor from the notebook environment you would like to write to NWB\n",
    "# chosen_sorting_extractor = sorting_outputs[('rec0', 'ironclust')]\n",
    "# chosen_sorting_extractor = sorting_ensemble\n",
    "\n",
    "# quick_write(\n",
    "#   ced_file_path=bin_file,\n",
    "#  session_description=session_description,\n",
    "# session_start=session_start,\n",
    "# save_path=nwbfile_path,\n",
    "# sorting=sorting_curated,\n",
    "# recording_lfp=None,\n",
    "# overwrite=True\n",
    "# )\n",
    "\n",
    "# se.NwbRecordingExtractor.write_recording(recording_lfp, 'LFPs.nw')\n",
    "\n",
    "quick_write(\n",
    "    ced_file_path=bin_file,\n",
    "    session_description=session_description,\n",
    "    session_start=session_start,\n",
    "    save_path=nwbfile_path,\n",
    "    sorting=sorting_phy,\n",
    "    recording_lfp=None,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
